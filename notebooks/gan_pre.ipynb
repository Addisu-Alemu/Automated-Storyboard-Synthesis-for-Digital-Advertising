{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "# Set the directory path to the folders containing _preview.png and asset files\n",
    "data_dir = 'preview_data'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available and set the device accordingly\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "class CompositionDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir):\n",
    "        self.data_dir = data_dir\n",
    "        self.folders = [f for f in os.listdir(data_dir) if os.path.isdir(os.path.join(data_dir, f))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.folders)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        folder = self.folders[idx]\n",
    "        preview_path = os.path.join(self.data_dir, folder, '_preview.png')\n",
    "        asset_paths = [os.path.join(self.data_dir, folder, f) for f in os.listdir(os.path.join(self.data_dir, folder)) if f != '_preview.png']\n",
    "\n",
    "        # Load the _preview.png image\n",
    "        preview_img = self.load_preview_image(preview_path)\n",
    "        \n",
    "        # Load the asset images\n",
    "        asset_imgs = []\n",
    "        for path in asset_paths:\n",
    "            try:\n",
    "                img = self.load_image(path)\n",
    "                asset_imgs.append(img)\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading asset image {path}: {str(e)}\")\n",
    "\n",
    "        if not asset_imgs:\n",
    "            # Raise an exception to stop the DataLoader\n",
    "            raise ValueError(f\"No asset images found for folder {folder}.\") \n",
    "\n",
    "        # Concatenate asset_imgs along the channel dimension\n",
    "        asset_tensor = torch.cat(asset_imgs, dim=0)\n",
    "\n",
    "        # Pad or truncate asset_tensor to fixed number of channels\n",
    "        target_channels = 48  # or whatever maximum number of channels you expect\n",
    "        if asset_tensor.size(0) < target_channels:\n",
    "            padding = torch.zeros(target_channels - asset_tensor.size(0), 256, 256)\n",
    "            asset_tensor = torch.cat([asset_tensor, padding], dim=0)\n",
    "        elif asset_tensor.size(0) > target_channels:\n",
    "            asset_tensor = asset_tensor[:target_channels]\n",
    "\n",
    "        #print(f\"Folder: {folder}\")\n",
    "        #print(f\"Number of asset images: {len(asset_imgs)}\")\n",
    "        #print(\"Preview image shape:\", preview_img.shape)\n",
    "        #print(\"Asset tensor shape:\", asset_tensor.shape)\n",
    "\n",
    "        return preview_img, asset_tensor\n",
    "    \n",
    "    def load_preview_image(self, path):\n",
    "        img = Image.open(path)\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        img_tensor = transform(img)\n",
    "        \n",
    "        # Convert grayscale to RGB if necessary\n",
    "        if img_tensor.shape[0] == 1:\n",
    "            img_tensor = img_tensor.repeat(3, 1, 1)\n",
    "        \n",
    "        # Normalize the image\n",
    "        normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        img_tensor = normalize(img_tensor)\n",
    "        \n",
    "        return img_tensor  # Shape: [3, 256, 256]\n",
    "\n",
    "    def load_image(self, path):\n",
    "        img = Image.open(path)\n",
    "        transform = transforms.Compose([\n",
    "            transforms.Resize((256, 256)),\n",
    "            transforms.ToTensor(),\n",
    "        ])\n",
    "        img_tensor = transform(img)\n",
    "        \n",
    "        # Convert grayscale to RGB if necessary\n",
    "        if img_tensor.shape[0] == 1:\n",
    "            img_tensor = img_tensor.repeat(3, 1, 1)\n",
    "        \n",
    "        # Normalize the image\n",
    "        normalize = transforms.Normalize(mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5])\n",
    "        img_tensor = normalize(img_tensor)\n",
    "        \n",
    "        return img_tensor  # Shape: [3, 256, 256]\n",
    "\n",
    "# Create a data loader from the custom dataset\n",
    "dataset = CompositionDataset(data_dir)\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the CycleGAN model\n",
    "class CycleGAN(nn.Module):\n",
    "    def __init__(self, in_channels_A, in_channels_B, out_channels_A, out_channels_B, num_residual_blocks):\n",
    "        super(CycleGAN, self).__init__()\n",
    "        self.generator_A2B = Generator(in_channels_A, out_channels_B, num_residual_blocks)\n",
    "        self.generator_B2A = Generator(in_channels_B, out_channels_A, num_residual_blocks)\n",
    "        self.discriminator_A = Discriminator(in_channels_A)\n",
    "        self.discriminator_B = Discriminator(in_channels_B)\n",
    "\n",
    "    def forward(self, x_A, x_B):\n",
    "        out_A2B = self.generator_A2B(x_A)\n",
    "        out_B2A = self.generator_B2A(x_B)\n",
    "        return out_A2B, out_B2A\n",
    "\n",
    "    def discriminator_forward(self, x_A, x_B):\n",
    "        out_A = self.discriminator_A(x_A)\n",
    "        out_B = self.discriminator_B(x_B)\n",
    "        return out_A, out_B\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_residual_blocks):\n",
    "        super(Generator, self).__init__()\n",
    "        self.initial = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, 64, kernel_size=7, padding=3),\n",
    "            nn.InstanceNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        # Down-sampling\n",
    "        self.down_blocks = nn.Sequential(\n",
    "            self._block(64, 128, 3, 2, 1),\n",
    "            self._block(128, 256, 3, 2, 1)\n",
    "        )\n",
    "        \n",
    "        # Residual blocks\n",
    "        self.res_blocks = nn.Sequential(\n",
    "            *[ResidualBlock(256) for _ in range(num_residual_blocks)]\n",
    "        )\n",
    "        \n",
    "        # Up-sampling\n",
    "        self.up_blocks = nn.Sequential(\n",
    "            self._block(256, 128, 3, 2, 1, upsample=True),\n",
    "            self._block(128, 64, 3, 2, 1, upsample=True)\n",
    "        )\n",
    "        \n",
    "        self.last = nn.Conv2d(64, out_channels, kernel_size=7, padding=3)\n",
    "        \n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding, upsample=False):\n",
    "        layers = []\n",
    "        if upsample:\n",
    "            layers.append(nn.Upsample(scale_factor=2, mode='nearest'))\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=padding))\n",
    "        else:\n",
    "            layers.append(nn.Conv2d(in_channels, out_channels, kernel_size, stride=stride, padding=padding))\n",
    "        layers.append(nn.InstanceNorm2d(out_channels))\n",
    "        layers.append(nn.ReLU(inplace=True))\n",
    "        return nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.initial(x)\n",
    "        x = self.down_blocks(x)\n",
    "        x = self.res_blocks(x)\n",
    "        x = self.up_blocks(x)\n",
    "        return self.last(x)\n",
    "\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv2d(channels, channels, kernel_size=3, padding=1)\n",
    "        self.instance_norm = nn.InstanceNorm2d(channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.instance_norm(self.conv1(x)))\n",
    "        out = self.instance_norm(self.conv2(out))\n",
    "        out += residual\n",
    "        return self.relu(out)\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        def discriminator_block(in_filters, out_filters, normalize=True):\n",
    "            layers = [nn.Conv2d(in_filters, out_filters, 4, stride=2, padding=1)]\n",
    "            if normalize:\n",
    "                layers.append(nn.InstanceNorm2d(out_filters))\n",
    "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
    "            return layers\n",
    "\n",
    "        self.model = nn.Sequential(\n",
    "            *discriminator_block(in_channels, 64, normalize=False),\n",
    "            *discriminator_block(64, 128),\n",
    "            *discriminator_block(128, 256),\n",
    "            *discriminator_block(256, 512),\n",
    "            nn.ZeroPad2d((1, 0, 1, 0)),\n",
    "            nn.Conv2d(512, 1, 4, padding=1),\n",
    "            nn.Sigmoid()  # Add sigmoid activation here\n",
    "        )\n",
    "\n",
    "    def forward(self, img):\n",
    "        return self.model(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the model and optimizers\n",
    "in_channels_B = 48  # 16 assets * 3 channels each\n",
    "model = CycleGAN(in_channels_A=3, in_channels_B=in_channels_B, out_channels_A=3, out_channels_B=in_channels_B, num_residual_blocks=6)\n",
    "model = model.to(device)\n",
    "\n",
    "optimizer_G = optim.Adam(list(model.generator_A2B.parameters()) + list(model.generator_B2A.parameters()), lr=0.001)\n",
    "optimizer_D_A = optim.Adam(model.discriminator_A.parameters(), lr=0.001)\n",
    "optimizer_D_B = optim.Adam(model.discriminator_B.parameters(), lr=0.001)\n",
    "\n",
    "# Define the loss functions\n",
    "criterion_GAN = nn.BCELoss()\n",
    "criterion_cycle = nn.L1Loss()\n",
    "criterion_identity = nn.L1Loss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "for epoch in range(100):\n",
    "    for i, (preview_img, asset_tensor) in enumerate(dataloader):\n",
    "        # Move data to device (GPU or CPU)\n",
    "        preview_img, asset_tensor = preview_img.to(device), asset_tensor.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        out_A2B, out_B2A = model(preview_img, asset_tensor)\n",
    "\n",
    "        # Calculate losses\n",
    "        # --- Discriminator A ---\n",
    "        out_D_A_real = model.discriminator_A(preview_img)\n",
    "        out_D_A_fake = model.discriminator_A(out_B2A.detach())  # Detach to avoid backpropagating through generator\n",
    "        out_D_A_real = out_D_A_real.view(out_D_A_real.size(0), -1)  # Flatten\n",
    "        out_D_A_fake = out_D_A_fake.view(out_D_A_fake.size(0), -1)\n",
    "        \n",
    "        # Reshape target labels to match flattened discriminator output\n",
    "        target_real = torch.ones(out_D_A_real.size()).to(device)\n",
    "        target_fake = torch.zeros(out_D_A_fake.size()).to(device)\n",
    "\n",
    "        loss_D_A_real = criterion_GAN(out_D_A_real, target_real)\n",
    "        loss_D_A_fake = criterion_GAN(out_D_A_fake, target_fake)\n",
    "        loss_D_A = (loss_D_A_real + loss_D_A_fake) * 0.5\n",
    "\n",
    "        # --- Discriminator B ---\n",
    "        out_D_B_real = model.discriminator_B(asset_tensor)\n",
    "        out_D_B_fake = model.discriminator_B(out_A2B.detach())\n",
    "        out_D_B_real = out_D_B_real.view(out_D_B_real.size(0), -1)\n",
    "        out_D_B_fake = out_D_B_fake.view(out_D_B_fake.size(0), -1)\n",
    "\n",
    "        loss_D_B_real = criterion_GAN(out_D_B_real, target_real)\n",
    "        loss_D_B_fake = criterion_GAN(out_D_B_fake, target_fake)\n",
    "        loss_D_B = (loss_D_B_real + loss_D_B_fake) * 0.5\n",
    "        # --- Generator ---\n",
    "        out_D_B_A2B = model.discriminator_B(out_A2B)  # Get discriminator output\n",
    "        out_D_B_A2B = out_D_B_A2B.view(out_D_B_A2B.size(0), -1)  # Flatten\n",
    "        loss_GAN_A2B = criterion_GAN(out_D_B_A2B, target_real)\n",
    "\n",
    "        out_D_A_B2A = model.discriminator_A(out_B2A)\n",
    "        out_D_A_B2A = out_D_A_B2A.view(out_D_A_B2A.size(0), -1)\n",
    "        loss_GAN_B2A = criterion_GAN(out_D_A_B2A, target_real)\n",
    "\n",
    "        loss_cycle_A = criterion_cycle(out_B2A, preview_img)\n",
    "        loss_cycle_B = criterion_cycle(out_A2B, asset_tensor)\n",
    "\n",
    "        loss_identity_A = criterion_identity(out_B2A, preview_img)\n",
    "        loss_identity_B = criterion_identity(out_A2B, asset_tensor)\n",
    "\n",
    "        loss_G = loss_GAN_A2B + loss_GAN_B2A + 10 * (loss_cycle_A + loss_cycle_B) + 5 * (loss_identity_A + loss_identity_B)\n",
    "\n",
    "        # Backward pass\n",
    "        optimizer_G.zero_grad()\n",
    "        loss_G.backward()\n",
    "        optimizer_G.step()\n",
    "\n",
    "        # Train discriminators\n",
    "        optimizer_D_A.zero_grad()\n",
    "        loss_D_A.backward()\n",
    "        optimizer_D_A.step()\n",
    "\n",
    "        optimizer_D_B.zero_grad()\n",
    "        loss_D_B.backward()\n",
    "        optimizer_D_B.step()\n",
    "\n",
    "        # Print lossesEpoch [1/100], Step [1/153], Loss_G: 21.1746, Loss_D_A: 0.7167, Loss_D_B: 0.7078\n",
    "        if i % 10 == 0:  # Print every 10 iterations\n",
    "            print(f'Epoch [{epoch+1}/100], Step [{i+1}/{len(dataloader)}], Loss_G: {loss_G.item():.4f}, Loss_D_A: {loss_D_A.item():.4f}, Loss_D_B: {loss_D_B.item():.4f}')\n",
    "\n",
    " \n",
    "    # Optional: Save the model after each epoch\n",
    "    # torch.save(model.state_dict(), f'cyclegan_model_epoch_{epoch+1}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Use the trained model to compose new images\n",
    "# def compose_images(assets):\n",
    "#     # Stack the asset images into a single tensor\n",
    "#     asset_tensor = torch.stack(assets)\n",
    "\n",
    "#     # Use the trained model to generate a composed image\n",
    "#     output = model(asset_tensor, asset_tensor)\n",
    "#     return output\n",
    "\n",
    "# # Example usage:\n",
    "# assets = [torchvision.load_image('asset1.png'), torchvision.load_image('asset2.png'), ...]\n",
    "# composed_img = compose_images(assets)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
